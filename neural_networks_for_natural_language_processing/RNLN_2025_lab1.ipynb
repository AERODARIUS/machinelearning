{"cells":[{"cell_type":"markdown","metadata":{"id":"t9ZNGOygrFsT"},"source":["# Laboratorio 1\n","**Redes Neuronales para Lenguaje Natural, 2025**\n","\n","En este laboratorio construiremos analizadores de sentimiento de tres clases (positivo, negativo, neutro) en textos que muestran opiniones, con el objetivo de comparar distintas técnicas de representación de los textos y utilizarlas para alimentar un modelo de redes neuronales.\n","\n","**Entrega: 9 de octubre de 2025**\n","\n","**Formato: notebook de Python (.ipynb)**\n","\n","**No olvidar mantener todas las salidas de cada región de código en el notebook!**\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cc9wopipf1gD"},"source":["## 0. Preparación del entorno"]},{"cell_type":"markdown","metadata":{"id":"jGKhZr0-Yx9O"},"source":["Comenzamos con algunos imports y definiciones."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sdk0m1WfYoeW"},"outputs":[],"source":["! pip install --upgrade gensim\n","\n","import torch\n","import gensim\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix\n","\n","POLARITY_LABELS = ['Neg','Pos','Neu']\n","POLARITY_ID = {p:i for (i,p) in enumerate(POLARITY_LABELS) }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R4IY8g42ILdB"},"outputs":[],"source":["POLARITY_ID"]},{"cell_type":"markdown","metadata":{"id":"-wlcQnjW3PqL"},"source":["Descargamos los datos de textos de prensa, anotados con una de las siguientes clases: Pos, Neg,Neu. Por más información sobre el dataset, consulte [aquí](https://github.com/pln-fing-udelar/pln-inco-resources/tree/master/sentiment/corpusAnalisisSentimientosEsp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u34lygcpDC1m"},"outputs":[],"source":["! wget https://raw.githubusercontent.com/pln-fing-udelar/pln-inco-resources/master/sentiment/corpusAnalisisSentimientosEsp/prensaUyUnaClase.csv"]},{"cell_type":"markdown","metadata":{"id":"EY1akF6ON9UF"},"source":["Cargamos el dataset en un dataframe de pandas y generamos conjuntos de entrenamiento, desarrollo y testeo.\n","Este dataset está compuesto por textos con opiniones. Para cada opinión tenemos el texto, un campo que no utilizaremos, y una categoría, que puede ser: Pos, Neg, o Neutro (indicando su polaridad)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lgs3RHq1gARr"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","dataset_df = pd.read_csv('./prensaUyUnaClase.csv', header=None)\n","dataset_df.columns = ['text', 'extra', 'polarity']\n","\n","# Dividimos train y test\n","temp_df, test_df = train_test_split(dataset_df, test_size=0.2, random_state=42)\n","\n","# Separamos conjunto de validación\n","train_df, dev_df = train_test_split(temp_df, test_size=0.2, random_state=42)\n","\n","print(\"Training set size:\", len(train_df))\n","print(\"Development set size:\", len(dev_df))\n","print(\"Test set size:\", len(test_df))\n","\n","display(dataset_df.iloc[0].text)\n","display(dataset_df.iloc[0].polarity)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kFox_v-5P9UM"},"source":["En este laboratorio utilizaremos el texto como entrada (columna \"text\"), y el valor a predecir será la polaridad (columna \"polarity\").\n","\n","Imprimimos algunos textos de ejemplo y sus categorías, y luego imprimimos la cantidad de ejemplos de las tres clases en las tres particiones."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSGVn-fjNwNe"},"outputs":[],"source":["train_text = train_df.loc[:,'text'].to_numpy()\n","dev_text = dev_df.loc[:,'text'].to_numpy()\n","test_text = test_df.loc[:,'text'].to_numpy()\n","\n","train_labels = np.array([POLARITY_ID[l] for l in train_df.loc[:,'polarity']])\n","dev_labels = np.array([POLARITY_ID[l] for l in dev_df.loc[:,'polarity']])\n","test_labels = np.array([POLARITY_ID[l] for l in test_df.loc[:,'polarity']])\n","\n","print(train_text[121])\n","print(POLARITY_LABELS[train_labels[121]])\n","\n","print(train_text[27])\n","print(POLARITY_LABELS[train_labels[27]])\n","\n","print(train_text[755])\n","print(POLARITY_LABELS[train_labels[755]])\n","\n","print(*POLARITY_LABELS, sep='\\t')\n","print(*np.bincount(train_labels), sep='\\t')\n","print(*np.bincount(dev_labels), sep='\\t')\n","print(*np.bincount(test_labels), sep='\\t')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mAEmfm-kIXjw"},"outputs":[],"source":["print(train_text[1])\n","print(train_labels[1])"]},{"cell_type":"markdown","metadata":{"id":"7sKveWCBgHyq"},"source":["## Parte 1"]},{"cell_type":"markdown","metadata":{"id":"aUVQ4dkXQYtf"},"source":["Utilizando pytorch, construir un clasificador tipo Multi-Layered Perceptron (MLP) que clasifique los textos según su sentimiento.\n","En esta parte, se debe utilizar una representación tipo **bag-of-words (BOW)** para los tweets, la cual se utilizará como entrada de la red.\n","\n","Puede probar realizar diferentes alternativas dentro de la representación BOW, por ejemplo considerar o no mayúsculas y minúsculas, descartar palabras con listas de stop words, usar N-gramas de orden mayor en vez de palabras simples, o utilizar tf-idf\n","\n","**Sugerencias:**\n","* Utilizar la clase CountVectorizer de sklearn\n","* Limitar el número máximo de features para no quedarse sin memoria\n","* Reducir la dimensionalidad del vector utilizando, por ejemplo, SVD"]},{"cell_type":"markdown","metadata":{"id":"krhvVHLeUqo5"},"source":["Describa las características de su mejor modelo, incluyendo arquitectura e hiperparámetros. Evalúelo sobre el corpus de **dev**, imprimiendo la métrica accuracy, y las métricas macro-precision, macro-recall y macro-F1. Incluya también una matriz de confusión.\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TgT4CAdSU5j9"},"outputs":[],"source":["# su código aquí\n"]},{"cell_type":"markdown","metadata":{"id":"IWtsONirghBO"},"source":["**Descripción del modelo**:\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"X5YeUOn2gtL8"},"source":["## Parte 2"]},{"cell_type":"markdown","metadata":{"id":"5klZ-zH3RuSx"},"source":["Utilizando pytorch, construir un clasificador tipo Multi-Layered Perceptron (MLP) que clasifique los textos según su polaridad.\n","En esta parte, se debe utilizar una representación tipo **centroide de word embeddings** para los tweets, la cual se utilizará como entrada de la red.\n","\n","Puede probar realizar diferentes alternativas dentro de la representación con word embeddings, por ejemplo considerar o no mayúsculas y minúsculas, o descartar palabras con listas de stop words.\n","\n","**Sugerencias:**\n","* Puede utilizar uno de los embeddings que vimos en el taller en clase\n","* O puede bajar alguna otra colección de embeddings, pero no entrene sus propios embeddings!"]},{"cell_type":"markdown","metadata":{"id":"EHS6K-O-UdKw"},"source":["## 2.1 Configuración y setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h4DykvKIUkOJ"},"outputs":[],"source":["import random\n","import numpy as np\n","import torch\n","from torch import nn\n","\n","from torch.utils.data import Dataset, DataLoader\n","from gensim.models import KeyedVectors, fasttext\n","from gensim.utils import simple_preprocess\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix\n","from typing import List, Dict, Optional\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Reproducibilidad\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","# Usar GPU si está disponible\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","\n","# Forzando la conversión a string por las dudas\n","train_text = np.array(train_text, dtype=str)\n","dev_text   = np.array(dev_text, dtype=str)\n","test_text  = np.array(test_text, dtype=str)\n","\n","# Forzando la conversión a enteros por las dudas\n","train_labels = np.asarray(train_labels, dtype=np.int64)\n","dev_labels   = np.asarray(dev_labels, dtype=np.int64)\n","test_labels  = np.asarray(test_labels, dtype=np.int64)\n"]},{"cell_type":"markdown","metadata":{"id":"oEaGrPoFZJhG"},"source":["## 2.2 Tokenizar y vectorizar datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3W3UZiL1pQt1"},"outputs":[],"source":["# -----------------------------\n","# 1) Tokenizador simple\n","# -----------------------------\n","def tokenize(text: str) -> List[str]:\n","    # minúsculas + sin signos, tokens de 2..30 chars\n","    return simple_preprocess(str(text), deacc=True, min_len=2, max_len=30)"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"djJ6PsXfpX5Y","executionInfo":{"status":"ok","timestamp":1759679581059,"user_tz":180,"elapsed":574422,"user":{"displayName":"Darío Cruz","userId":"02178488993545239579"}},"outputId":"0da7806a-aedb-45a8-90d5-6fc4d473e0b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-10-05 15:43:26--  https://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/SBW-vectors-300-min5.bin.gz\n","Resolving cs.famaf.unc.edu.ar (cs.famaf.unc.edu.ar)... 200.16.17.55\n","Connecting to cs.famaf.unc.edu.ar (cs.famaf.unc.edu.ar)|200.16.17.55|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1123304474 (1.0G) [application/x-gzip]\n","Saving to: ‘SBW-vectors-300-min5.bin.gz’\n","\n","SBW-vectors-300-min 100%[===================>]   1.05G  16.1MB/s    in 58s     \n","\n","2025-10-05 15:44:25 (18.6 MB/s) - ‘SBW-vectors-300-min5.bin.gz’ saved [1123304474/1123304474]\n","\n","gzip: SBW-vectors-300-min5.bin already exists; do you wish to overwrite (y or n)? y\n","\n"]}],"source":["\n","# ------------------------------------\n","# Descargar ebeddings y cargar modelo\n","# ------------------------------------\n","# 2) Cargar embeddings *preentrenados* (NO entrenar nada)\n","# Usamos SBWC (más liviano, pero sin OOV):\n","!wget https://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/SBW-vectors-300-min5.bin.gz\n","!gunzip SBW-vectors-300-min5.bin.gz\n","kv = KeyedVectors.load_word2vec_format(\"SBW-vectors-300-min5.bin\", binary=True)\n","\n","\n","embed_dim = kv.vector_size"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":212,"status":"ok","timestamp":1759679581276,"user":{"displayName":"Darío Cruz","userId":"02178488993545239579"},"user_tz":180},"id":"XmDPFHxEx-rt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"84cc819f-1b3e-45cb-a26c-ffda048b2d3c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]}],"source":["# -----------------------------\n","# 3) TF-IDF para ponderar el centroide\n","# -----------------------------\n","# tfidf = TfidfVectorizer(tokenizer=tokenize, lowercase=False, min_df=2)  # ajusta min_df si el corpus es pequeño\n","# tfidf.fit(train_text)\n","tfidf = TfidfVectorizer(tokenizer=tokenize, lowercase=False, min_df=2)\n","tfidf.fit(train_text)\n","\n","idf_vals = tfidf.idf_\n","vocab = tfidf.vocabulary_\n","idf = {t: idf_vals[i] for t, i in vocab.items()}\n","\n","# Diccionario token -> idf\n","idf_vals = tfidf.idf_\n","vocab = tfidf.vocabulary_            # token -> idx\n","idf: Dict[str, float] = {t: idf_vals[i] for t, i in vocab.items()}\n"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":680,"status":"ok","timestamp":1759679581981,"user":{"displayName":"Darío Cruz","userId":"02178488993545239579"},"user_tz":180},"id":"uiLPuOeAZdDF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"628ffcd8-67bf-483f-b639-074a58457657"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","(806, 300)\n","(202, 300)\n","(253, 300)\n"]}],"source":["# -----------------------------\n","# 4) Centroide (promedio ponderado por TF-IDF)\n","# -----------------------------\n","def doc_centroid(tokens, normalize: bool = True):\n","    # pesos TF-IDF (fallback 1.0 si no está en el vocab del TF-IDF)\n","    w = [idf.get(t, 1.0) for t in tokens]\n","    try:\n","        # pre_normalize/post_normalize controlan normalizaciones internas\n","        vec = kv.get_mean_vector(tokens, weights=w,\n","                                 pre_normalize=False,\n","                                 post_normalize=False)\n","    except KeyError:\n","        # (por si alguna versión lanza error con OOV)\n","        vec = np.zeros(embed_dim, dtype=np.float32)\n","\n","    if normalize:\n","        n = np.linalg.norm(vec)\n","        if n > 0:\n","            vec = vec / n\n","    return vec.astype(np.float32)\n","\n","\n","def vectorize_texts(texts: List[str]) -> np.ndarray:\n","    return np.vstack([doc_centroid(tokenize(t)) for t in texts])\n","\n","# -----------------------------\n","# 5) Vectorizar splits\n","# -----------------------------\n","X_train = vectorize_texts(train_text)\n","X_dev   = vectorize_texts(dev_text)\n","X_test  = vectorize_texts(test_text)\n","\n","print()\n","print(X_train.shape)\n","print(X_dev.shape)\n","print(X_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"TJ2Xaoi6Utjl"},"source":["## 2.3 Entrenamiento de embeddings"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":50,"status":"ok","timestamp":1759679582034,"user":{"displayName":"Darío Cruz","userId":"02178488993545239579"},"user_tz":180},"id":"wSjCarkUiLcW"},"outputs":[],"source":["# Escalamos (fit en train, transform en dev/test) — ayuda a MLP\n","scaler = StandardScaler(with_mean=True, with_std=True)\n","X_train = scaler.fit_transform(X_train).astype(np.float32)\n","X_dev   = scaler.transform(X_dev).astype(np.float32)\n","X_test  = scaler.transform(X_test).astype(np.float32)"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1759679582041,"user":{"displayName":"Darío Cruz","userId":"02178488993545239579"},"user_tz":180},"id":"A_zT_bNAiPU2"},"outputs":[],"source":["# ====== Datasets y DataLoaders ======\n","class NpDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = torch.from_numpy(X).float()\n","        self.y = torch.from_numpy(y).long()\n","    def __len__(self): return len(self.y)\n","    def __getitem__(self, idx): return self.X[idx], self.y[idx]"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1759679582047,"user":{"displayName":"Darío Cruz","userId":"02178488993545239579"},"user_tz":180},"id":"nE2OsrpiiQ0G"},"outputs":[],"source":["batch_size = 64\n","train_ds = NpDataset(X_train, train_labels)\n","dev_ds   = NpDataset(X_dev, dev_labels)\n","test_ds  = NpDataset(X_test, test_labels)\n","\n","train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n","dev_loader   = DataLoader(dev_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n","test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, drop_last=False)"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1759679582055,"user":{"displayName":"Darío Cruz","userId":"02178488993545239579"},"user_tz":180},"id":"dv9JBTNjiYeh"},"outputs":[],"source":["# ====== MLP ======\n","class MLP(nn.Module):\n","    def __init__(self, input_dim, hidden=(256, 128), num_classes=3, dropout=0.3):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, hidden[0]),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden[0], hidden[1]),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden[1], num_classes),\n","        )\n","    def forward(self, x): return self.net(x)\n","\n","model = MLP(input_dim=embed_dim, hidden=(256,128), num_classes=len(POLARITY_LABELS), dropout=0.3).to(device)"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1759679582061,"user":{"displayName":"Darío Cruz","userId":"02178488993545239579"},"user_tz":180},"id":"3_WVzbQXiexI"},"outputs":[],"source":["# Ponderación de clases (por si hay desbalance)\n","counts = np.bincount(train_labels, minlength=len(POLARITY_LABELS))\n","inv_freq = 1.0 / np.clip(counts, 1, None)\n","class_weights = torch.tensor(inv_freq / inv_freq.sum() * len(inv_freq), dtype=torch.float32, device=device)\n","\n","criterion = nn.CrossEntropyLoss(weight=class_weights)  # ponderado\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":607,"status":"ok","timestamp":1759679582674,"user":{"displayName":"Darío Cruz","userId":"02178488993545239579"},"user_tz":180},"id":"Bp4-9iw8ij8f","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ce941110-3198-4fd2-bbb0-8eb2b9f45aaf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 01 | Train loss 1.0593 acc 0.4293 | Dev loss 1.0119 acc 0.5099\n","Epoch 02 | Train loss 0.8975 acc 0.6179 | Dev loss 0.9446 acc 0.5396\n","Epoch 03 | Train loss 0.7407 acc 0.6836 | Dev loss 0.9325 acc 0.5743\n","Epoch 04 | Train loss 0.6052 acc 0.7531 | Dev loss 0.9573 acc 0.5693\n","Epoch 05 | Train loss 0.4953 acc 0.7829 | Dev loss 1.0085 acc 0.5495\n","Epoch 06 | Train loss 0.3813 acc 0.8598 | Dev loss 1.1487 acc 0.5842\n","Epoch 07 | Train loss 0.2901 acc 0.8995 | Dev loss 1.2214 acc 0.5495\n","Epoch 08 | Train loss 0.1964 acc 0.9541 | Dev loss 1.3011 acc 0.5693\n","Epoch 09 | Train loss 0.1586 acc 0.9541 | Dev loss 1.4556 acc 0.5297\n","Epoch 10 | Train loss 0.1038 acc 0.9739 | Dev loss 1.5905 acc 0.5396\n","Early stopping.\n"]}],"source":["# ====== Entrenamiento con early-stopping ======\n","best_dev_loss = float(\"inf\")\n","best_state = None\n","patience = 7\n","epochs = 50\n","best_epoch = -1\n","no_improve = 0\n","\n","def run_epoch(dloader, train_mode=True):\n","    if train_mode:\n","        model.train()\n","    else:\n","        model.eval()\n","    total_loss = 0.0\n","    total_correct = 0\n","    total = 0\n","    for Xb, yb in dloader:\n","        Xb = Xb.to(device)\n","        yb = yb.to(device)\n","        if train_mode:\n","            optimizer.zero_grad()\n","        with torch.set_grad_enabled(train_mode):\n","            logits = model(Xb)\n","            loss = criterion(logits, yb)\n","            if train_mode:\n","                loss.backward()\n","                optimizer.step()\n","        total_loss += loss.item() * yb.size(0)\n","        preds = torch.argmax(logits, dim=1)\n","        total_correct += (preds == yb).sum().item()\n","        total += yb.size(0)\n","    return total_loss / total, total_correct / total\n","\n","for epoch in range(1, epochs+1):\n","    tr_loss, tr_acc = run_epoch(train_loader, train_mode=True)\n","    dv_loss, dv_acc = run_epoch(dev_loader, train_mode=False)\n","\n","    if dv_loss < best_dev_loss - 1e-5:\n","        best_dev_loss = dv_loss\n","        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n","        best_epoch = epoch\n","        no_improve = 0\n","    else:\n","        no_improve += 1\n","\n","    print(f\"Epoch {epoch:02d} | Train loss {tr_loss:.4f} acc {tr_acc:.4f} | Dev loss {dv_loss:.4f} acc {dv_acc:.4f}\")\n","    if no_improve >= patience:\n","        print(\"Early stopping.\")\n","        break"]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":67,"status":"ok","timestamp":1759679582744,"user":{"displayName":"Darío Cruz","userId":"02178488993545239579"},"user_tz":180},"id":"81cqcXZCipM6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e8f69a71-41dd-4af4-d116-1e75346d5f07"},"outputs":[{"output_type":"stream","name":"stdout","text":["best_epoch = 3\n"]},{"output_type":"execute_result","data":{"text/plain":["MLP(\n","  (net): Sequential(\n","    (0): Linear(in_features=300, out_features=256, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.3, inplace=False)\n","    (3): Linear(in_features=256, out_features=128, bias=True)\n","    (4): ReLU()\n","    (5): Dropout(p=0.3, inplace=False)\n","    (6): Linear(in_features=128, out_features=3, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":46}],"source":["# Cargamos el mejor estado\n","print(f'best_epoch = {best_epoch}')\n","if best_state is not None:\n","    model.load_state_dict(best_state)\n","model.to(device)"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":206,"status":"ok","timestamp":1759679582952,"user":{"displayName":"Darío Cruz","userId":"02178488993545239579"},"user_tz":180},"id":"6TKrfFb8T6LX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e149e198-674f-41db-a0f8-9cf16feaff87"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","== DEV ==\n","Accuracy: 0.5743\n","Per-class (Neg, Pos, Neu) -> precision | recall | f1\n","Neg: 0.635 | 0.659 | 0.647\n","Pos: 0.571 | 0.581 | 0.576\n","Neu: 0.481 | 0.448 | 0.464\n","Confusion Matrix (rows=true, cols=pred):\n","[[54 13 15]\n"," [13 36 13]\n"," [18 14 26]]\n","\n","== TEST ==\n","Accuracy: 0.6047\n","Per-class (Neg, Pos, Neu) -> precision | recall | f1\n","Neg: 0.735 | 0.632 | 0.679\n","Pos: 0.620 | 0.550 | 0.583\n","Neu: 0.440 | 0.627 | 0.517\n","Confusion Matrix (rows=true, cols=pred):\n","[[72 17 25]\n"," [14 44 22]\n"," [12 10 37]]\n"]}],"source":["# ====== Evaluación final (dev y test) ======\n","def predict_all(dloader):\n","    model.eval()\n","    preds_list, y_list = [], []\n","    with torch.no_grad():\n","        for Xb, yb in dloader:\n","            Xb = Xb.to(device)\n","            logits = model(Xb)\n","            preds = torch.argmax(logits, dim=1).cpu().numpy()\n","            preds_list.append(preds)\n","            y_list.append(yb.numpy())\n","    y_true = np.concatenate(y_list)\n","    y_pred = np.concatenate(preds_list)\n","    return y_true, y_pred\n","\n","def report_split(name, y_true, y_pred):\n","    acc = accuracy_score(y_true, y_pred)\n","    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=list(range(len(POLARITY_LABELS))), zero_division=0)\n","    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(POLARITY_LABELS))))\n","    print(f\"\\n== {name} ==\")\n","    print(f\"Accuracy: {acc:.4f}\")\n","    print(\"Per-class (Neg, Pos, Neu) -> precision | recall | f1\")\n","    for i, lab in enumerate(POLARITY_LABELS):\n","        print(f\"{lab:>3}: {pr[i]:.3f} | {rc[i]:.3f} | {f1[i]:.3f}\")\n","    print(\"Confusion Matrix (rows=true, cols=pred):\")\n","    print(cm)\n","\n","y_dev_true,  y_dev_pred  = predict_all(dev_loader)\n","y_test_true, y_test_pred = predict_all(test_loader)\n","\n","report_split(\"DEV\",  y_dev_true,  y_dev_pred)\n","report_split(\"TEST\", y_test_true, y_test_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"df0qDz-cFhUk"},"source":["## Observaciones:\n","Aprende muy rápido lo poco que puede generalizar y después empieza a empeorar (overfitting o mal ajuste de hiperparámetros).\n"]},{"cell_type":"markdown","metadata":{"id":"cy10PcMRU8Qq"},"source":["Describa las características de su mejor modelo, incluyendo arquitectura e hiperparámetros. Evalúelo sobre el corpus de **dev**, imprimiendo la métrica accuracy, y las métricas macro-precision, macro-recall y macro-F1. Incluya también una matriz de confusión.\n"]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1759679582955,"user":{"displayName":"Darío Cruz","userId":"02178488993545239579"},"user_tz":180},"id":"igl-xpy4U9pV"},"outputs":[],"source":["# su código aquí\n"]},{"cell_type":"markdown","metadata":{"id":"AClp-fF1g9ph"},"source":["# Descripción del modelo:\n","\n","```\n","Texto ──tokenize──┐\n","                  ├─ TF-IDF (IDF por token)\n","                  └─ fastText vectors\n","                         │\n","            (promedio ponderado por IDF)\n","                         │\n","                  Centroide (300,)\n","                         │\n","                StandardScaler\n","                         │\n","                      (B,300)\n","                         │\n","        ┌────────── MLP (entrenable) ──────────┐\n","        │ Linear 300→256 → ReLU → Dropout      │\n","        │ Linear 256→128 → ReLU → Dropout      │\n","        │ Linear 128→3                         │\n","        └──────────────────────────────────────┘\n","                         │\n","                      Logits\n","                         │\n","              CrossEntropyLoss (+ class weights)\n","```\n","\n","- Representación (centroide de embeddings): cada texto se transforma en el promedio de los vectores Word2Vec de sus palabras presentes en el vocabulario. Es simple, rápido y funciona sorprendentemente bien como baseline.\n","- MLP: dos capas ocultas (256→128), ReLU, Dropout=0.3, Adam (1e-3), weight_decay ligero (1e-4).\n","- Desbalance: se calculan pesos por clase a partir de la frecuencia en train para CrossEntropyLoss(weight=...).\n","- Entrenamiento: early-stopping por dev_loss con patience=7.\n","- Escalado: StandardScaler (fit en train) mejora la estabilidad del MLP."]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1759679582959,"user":{"displayName":"Darío Cruz","userId":"02178488993545239579"},"user_tz":180},"id":"7bI40CG_WdKh"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1759679582964,"user":{"displayName":"Darío Cruz","userId":"02178488993545239579"},"user_tz":180},"id":"iUZ71IA6hF4Q"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"dXdwVyuCNlUh"},"source":["Repita el paso anterior, utilizando la misma arquitectura, pero cambiando los embeddings utilizados. Compare los resultados y comente."]},{"cell_type":"code","source":["# 2.BIS)\n","# Otras opciones:\n","# - FASTTEXT (mejor para tweets con OOV, pero pesado)\n","# - SBWC (más liviano, pero sin OOV)\n","# -----------------------------\n","# Alaternativa A (recomendada pero consume mucha RAM): fastText .bin con OOV\n","# kv = fasttext.load_facebook_vectors(\"cc.es.300.bin\")  # FastTextKeyedVectors\n","\n","# Alternativa B (más liviana): Word2Vec/GloVe en formato word2vec .vec SIN OOV\n","! wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\n","! gunzip cc.es.300.vec.gz\n","kv = KeyedVectors.load_word2vec_format(\"cc.es.300.vec\", binary=False)\n","\n","# -----------------------------\n","# 3.BIS) TF-IDF para ponderar el centroide\n","# -----------------------------\n","# tfidf = TfidfVectorizer(tokenizer=tokenize, lowercase=False, min_df=2)  # ajusta min_df si el corpus es pequeño\n","# tfidf.fit(train_text)\n","tfidf = TfidfVectorizer(tokenizer=tokenize, lowercase=False, min_df=2)\n","tfidf.fit(train_text)\n","\n","idf_vals = tfidf.idf_\n","vocab = tfidf.vocabulary_\n","idf = {t: idf_vals[i] for t, i in vocab.items()}\n","\n","# Diccionario token -> idf\n","idf_vals = tfidf.idf_\n","vocab = tfidf.vocabulary_            # token -> idx\n","idf: Dict[str, float] = {t: idf_vals[i] for t, i in vocab.items()}\n","\n","# -----------------------------\n","# 4.BIS) Centroide (promedio ponderado por TF-IDF)\n","# -----------------------------\n","def doc_centroid(tokens, normalize: bool = True):\n","    # pesos TF-IDF (fallback 1.0 si no está en el vocab del TF-IDF)\n","    w = [idf.get(t, 1.0) for t in tokens]\n","    try:\n","        # pre_normalize/post_normalize controlan normalizaciones internas\n","        vec = kv.get_mean_vector(tokens, weights=w,\n","                                 pre_normalize=False,\n","                                 post_normalize=False)\n","    except KeyError:\n","        # (por si alguna versión lanza error con OOV)\n","        vec = np.zeros(embed_dim, dtype=np.float32)\n","\n","    if normalize:\n","        n = np.linalg.norm(vec)\n","        if n > 0:\n","            vec = vec / n\n","    return vec.astype(np.float32)\n","\n","\n","def vectorize_texts(texts: List[str]) -> np.ndarray:\n","    return np.vstack([doc_centroid(tokenize(t)) for t in texts])\n","\n","# -----------------------------\n","# 5.BIS) Vectorizar splits\n","# -----------------------------\n","X_train = vectorize_texts(train_text)\n","X_dev   = vectorize_texts(dev_text)\n","\n","print()\n","print(X_train.shape)\n","print(X_dev.shape)\n","\n","# -----------------------------\n","#  Entrenamiento de embeddings\n","# -----------------------------\n","\n","# ====== Datasets y DataLoaders ======\n","class NpDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = torch.from_numpy(X).float()\n","        self.y = torch.from_numpy(y).long()\n","    def __len__(self): return len(self.y)\n","    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n","\n","batch_size = 64\n","train_ds = NpDataset(X_train, train_labels)\n","dev_ds   = NpDataset(X_dev, dev_labels)\n","test_ds  = NpDataset(X_test, test_labels)\n","\n","train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n","dev_loader   = DataLoader(dev_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n","test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n","\n","# ====== MLP ======\n","class MLP(nn.Module):\n","    def __init__(self, input_dim, hidden=(256, 128), num_classes=3, dropout=0.3):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, hidden[0]),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden[0], hidden[1]),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden[1], num_classes),\n","        )\n","    def forward(self, x): return self.net(x)\n","\n","model = MLP(input_dim=embed_dim, hidden=(256,128), num_classes=len(POLARITY_LABELS), dropout=0.3).to(device)\n","\n","# Ponderación de clases (por si hay desbalance)\n","counts = np.bincount(train_labels, minlength=len(POLARITY_LABELS))\n","inv_freq = 1.0 / np.clip(counts, 1, None)\n","class_weights = torch.tensor(inv_freq / inv_freq.sum() * len(inv_freq), dtype=torch.float32, device=device)\n","\n","criterion = nn.CrossEntropyLoss(weight=class_weights)  # ponderado\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n","\n","# ====== Entrenamiento con early-stopping ======\n","best_dev_loss = float(\"inf\")\n","best_state = None\n","patience = 7\n","epochs = 50\n","best_epoch = -1\n","no_improve = 0\n","\n","def run_epoch(dloader, train_mode=True):\n","    if train_mode:\n","        model.train()\n","    else:\n","        model.eval()\n","    total_loss = 0.0\n","    total_correct = 0\n","    total = 0\n","    for Xb, yb in dloader:\n","        Xb = Xb.to(device)\n","        yb = yb.to(device)\n","        if train_mode:\n","            optimizer.zero_grad()\n","        with torch.set_grad_enabled(train_mode):\n","            logits = model(Xb)\n","            loss = criterion(logits, yb)\n","            if train_mode:\n","                loss.backward()\n","                optimizer.step()\n","        total_loss += loss.item() * yb.size(0)\n","        preds = torch.argmax(logits, dim=1)\n","        total_correct += (preds == yb).sum().item()\n","        total += yb.size(0)\n","    return total_loss / total, total_correct / total\n","\n","for epoch in range(1, epochs+1):\n","    tr_loss, tr_acc = run_epoch(train_loader, train_mode=True)\n","    dv_loss, dv_acc = run_epoch(dev_loader, train_mode=False)\n","\n","    if dv_loss < best_dev_loss - 1e-5:\n","        best_dev_loss = dv_loss\n","        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n","        best_epoch = epoch\n","        no_improve = 0\n","    else:\n","        no_improve += 1\n","\n","    print(f\"Epoch {epoch:02d} | Train loss {tr_loss:.4f} acc {tr_acc:.4f} | Dev loss {dv_loss:.4f} acc {dv_acc:.4f}\")\n","    if no_improve >= patience:\n","        print(\"Early stopping.\")\n","        break\n","\n","# Cargamos el mejor estado\n","print(f'best_epoch = {best_epoch}')\n","if best_state is not None:\n","    model.load_state_dict(best_state)\n","model.to(device)\n","\n","# ====== Evaluación final (dev y test) ======\n","def predict_all(dloader):\n","    model.eval()\n","    preds_list, y_list = [], []\n","    with torch.no_grad():\n","        for Xb, yb in dloader:\n","            Xb = Xb.to(device)\n","            logits = model(Xb)\n","            preds = torch.argmax(logits, dim=1).cpu().numpy()\n","            preds_list.append(preds)\n","            y_list.append(yb.numpy())\n","    y_true = np.concatenate(y_list)\n","    y_pred = np.concatenate(preds_list)\n","    return y_true, y_pred\n","\n","def report_split(name, y_true, y_pred):\n","    acc = accuracy_score(y_true, y_pred)\n","    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=list(range(len(POLARITY_LABELS))), zero_division=0)\n","    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(POLARITY_LABELS))))\n","    print(f\"\\n== {name} ==\")\n","    print(f\"Accuracy: {acc:.4f}\")\n","    print(\"Per-class (Neg, Pos, Neu) -> precision | recall | f1\")\n","    for i, lab in enumerate(POLARITY_LABELS):\n","        print(f\"{lab:>3}: {pr[i]:.3f} | {rc[i]:.3f} | {f1[i]:.3f}\")\n","    print(\"Confusion Matrix (rows=true, cols=pred):\")\n","    print(cm)\n","\n","y_dev_true,  y_dev_pred  = predict_all(dev_loader)\n","y_test_true, y_test_pred = predict_all(test_loader)\n","\n","report_split(\"DEV\",  y_dev_true,  y_dev_pred)\n","report_split(\"TEST\", y_test_true, y_test_pred)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"20yXzP6pK9UH","outputId":"6b74318e-06bc-4b03-f568-1efed1a7a91b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-10-05 15:53:03--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 99.84.41.80, 99.84.41.79, 99.84.41.129, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|99.84.41.80|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1285580896 (1.2G) [binary/octet-stream]\n","Saving to: ‘cc.es.300.vec.gz’\n","\n","cc.es.300.vec.gz    100%[===================>]   1.20G   221MB/s    in 5.0s    \n","\n","2025-10-05 15:53:08 (244 MB/s) - ‘cc.es.300.vec.gz’ saved [1285580896/1285580896]\n","\n","gzip: cc.es.300.vec already exists; do you wish to overwrite (y or n)? "]}]},{"cell_type":"markdown","metadata":{"id":"RqFZsXcWhDUd"},"source":["**Comentarios**:\n","La alternativa A no la pudimos probar porque consume más RAM de la que tenemos disponible para el laboratorio.\n","\n","TOOD:  comparar\n","..."]},{"cell_type":"markdown","metadata":{"id":"pK_cPTC1T79O"},"source":["## Parte 3 (opcional):\n","Elija uno de los clasificadores anteriores y realice una búsqueda automatizada de hiperparámetros del modelo. Por ejemplo, puede utilizar una búsqueda en grilla completa o una búsqueda aleatoria, eligiendo diferente cantidad de capas, número unidades por capa, funciones de activación, y valores de learning rate. Intente encontrar el mejor clasificador posible, comparándolos sobre el corpus de **dev**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kdL72afBVAbG"},"outputs":[],"source":["# su código aquí\n"]},{"cell_type":"markdown","metadata":{"id":"TSWMIx_tVDzo"},"source":["Describa las características de su mejor modelo, incluyendo arquitectura e hiperparámetros. Evalúelo sobre el corpus de **dev**, imprimiendo la métrica accuracy, y las métricas macro-precision, macro-recall y macro-F1. Incluya también una matriz de confusión."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d-pblGJkVH3_"},"outputs":[],"source":["# su código aquí\n"]},{"cell_type":"markdown","metadata":{"id":"GOYqBdbKVJfh"},"source":["## Parte 4:\n","\n","Elija el mejor clasificador de todos los construidos en las partes anteriores, comparándolos utilizando la medida macro-F1, y evalúelo sobre el corpus de **test**, imprimiendo la métrica accuracy, y las métricas macro-precision, macro-recall y macro-F1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GgJFRXTsVe8G"},"outputs":[],"source":["# su código aquí\n"]},{"cell_type":"markdown","metadata":{"id":"sESZGWIYyP3Y"},"source":["Despliege la matriz de confusión sobre el corpus de **test** para las tres clases (P, N, NEU) para el mejor clasificador construido en las partes anteriores."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YkkYI0LR39Of"},"outputs":[],"source":["# su código aquí\n"]},{"cell_type":"markdown","metadata":{"id":"L1D9IB7gVgmG"},"source":["Analice los resultados, y responda, al menos, las siguientes preguntas:\n","\n","1.   ¿Cuál fue la representación que funcionó mejor? Describa brevemente sus características.\n","2.   ¿Cuál es la forma del modelo? Indique cantidad de capas, unidades, y funciones de activación. Puede usar el método print() sobre el modelo para imprimir la forma del modelo.\n","3.   Indique qué categoría o categorías fueron las más difíciles de aprender para su clasificador. ¿La categoría más difícil es la misma para el mejor clasificador de las partes 1 y 2 (y la parte 3 si la hizo)?"]},{"cell_type":"markdown","metadata":{"id":"peHQat3XWzX2"},"source":["( sus respuestas aquí)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}